<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resources/style.css" media="screen"/>

<html lang="en">
<head>
  	<title>ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting</title>
      <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
          if you update and want to force Facebook to re-scrape. -->
  	<meta property="og:image" content="./resources/framework.png"/>
  	<meta property="og:title" content="Collaborative Manipulation with Cost-Aware Human Forecasting" />
  	<meta property="og:description" content="ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting" />
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta property="twitter:card"          content="summary" />
    <meta property="twitter:title"         content="Collaborative Manipulation with Cost-Aware Human Forecasting" />
    <meta property="twitter:description"   content="ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting" />
    <meta property="twitter:image"         content="./framework.png" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>

<body>
<div class="container"> 
    <div>
        <a href="https://portal.cs.cornell.edu/"><img id="logo" src="./resources/portal_light.png"
         alt="Portal Group Logo"/></a>
         <a href="https://portal.cs.cornell.edu/"><img id="cornell" src="./resources/Cornell_University_Logo.png"
            alt="Portal Group Logo"/></a>
         <!-- <img id="cornell" src="./resources/Cornell_University_Logo.png"
         alt="Cornell Logo"/> -->
    </div>


    <div class="title" style="font-size:40px;">
        ManiCast: Collaborative Manipulation <br>with Cost-Aware Human Forecasting
    </div>

    <div class="venue">
        CoRL 2023
    </div>

    <br><br>

    <div class="author">
        <a href="https://kushal2000.github.io/">Kushal Kedia</a><sup>*</sup>
    </div>
    <div class="author">
        <a href="https://portfolio-pdan101.vercel.app/">Prithwish Dan</a><sup>*</sup>
    </div>
    <div class="author">
        Atiksh Bhardwaj<sup>*</sup>
    </div>
    <div class="author">
        <a href="https://www.sanjibanchoudhury.com/">Sanjiban Choudhury</a><sup>*</sup>
    </div>


    <br><br>

    <div class="affiliation"><sup>*&nbsp;</sup>Cornell University</div>
    <!-- <div class="affiliation"><sup>2&nbsp;</sup>Affiliation Number Two</div>
    <div class="affiliation"><sup>3&nbsp;</sup>Affiliation Number Three</div>
    <div class="affiliation"><sup>4&nbsp;</sup>Affiliation Number Four</div> -->

    <br><br>

    <div class="links"><a href="https://openreview.net/forum?id=rxlokRzNWRq">[Paper]</a></div>
    <!-- <div class="links">[Paper]</div> -->
    <div class="links" style="max-width: 250px;"><a href="https://github.com/portal-cornell/manicast">[Code + Datasets]</a></div>
    <!-- <div class="links" style="max-width: 250px;">[Code (Coming Soon)]</div> -->
    <!-- <div class="links" style="max-width: 250px;">[Data (Coming Soon)]</div> -->

    <br><br>

    <img style="width: 90%;" src="./resources/approach_fig.png" alt="Method Overview."/>
    <br><br>
    
        <p style="max-width: 950px;width:90%;text-align: justify;text-justify: inter-word;">
            Overview of our framework <b>ManiCast</b>, 
            which learns <b>cost-aware human motion forecasts</b> and <b>plans with such forecasts</b>
            for <b>collaborative manipulation</b> tasks. At train time, we fine-tune pre-trained 
            human motion forecasting models on task specific datasets by upsampling 
            transition points and upweighting joint dimensions that dominate the cost 
            of the robot's planned trajectory. At inference time, we feed these forecasts 
            into a model predictive control (MPC) planner to compute robot plans that 
            are <b>reactive</b> and keep a <b>safe distance</b> from the human.
        </p>
        
    <br>
    <hr>

    <h1>Abstract</h1>
    <p style="width: 80%;text-align: justify;">
        Seamless human-robot manipulation in close proximity relies on accurate forecasts of human motion. 
        While there has been significant progress in learning forecast models at scale, when applied to manipulation tasks,
         these models accrue high errors at critical transition points leading to degradation in downstream planning performance. 
         Our key insight is that instead of predicting the most likely human motion, it is sufficient to produce forecasts that
          capture how future human motion would affect the cost of a robot's plan. We present ManiCast, 
          a novel framework that learns cost-aware human forecasts and feeds them to a model predictive control planner to execute 
          collaborative manipulation tasks. Our framework enables fluid, real-time interactions between a human and a 7-DoF robot arm 
          across a number of real-world tasks such as reactive stirring, object handovers, and collaborative table setting. 
          We evaluate both the motion forecasts and the end-to-end forecaster-planner system against a range of learned and 
          heuristic baselines while additionally contributing new datasets. 
    </p>

    <br><br>
    <hr>

    <h1>Video</h1>
    <div class="video-container">
        <iframe src="./resources/Spotlight.mp4" frameBorder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </div>

    <!-- <video width="800px";
    frameBorder="0"; 
    controls="controls";
    style="border:2px solid black";>
        <source src="./resources/video.mp4" type="video/mp4"></source>
    </video> -->

    <br><br><br><br>
    <hr>


    <h1>Human-Robot<br>Collaborative Manipulation</h1>
    <br>

    <img style="width: 90%;" src="./resources/Fig1_Final.png" alt="Collaborative Tasks"/>
    <p style="width: 80%;text-align: justify;margin-top: 0.8em;">
        We present closed-loop, real-time collaborative human-robot manipulation across three different
        kitchen tasks by combining learned human pose forecasts with model predictive control.
    </p>

    <br><br><br>

    <img style="width: 90%;" src="./resources/Fig3_Final.png" alt="Transition Windows"/>
    <p style="width: 80%;text-align: justify;margin-top: 0.8em;">
        (Top) The <i>x</i>-position of the reaching human's wrist in a Reactive Stirring test set episode. <i>x</i> &ge; 0.4 indicates the wrist is near the pot.
        <br>(Bottom) <i>Base</i> model's forecasts (trained only on large scale motion data) have high errors during transitions and lag behind the current pose. 
        <i>ManiCast</i>, trained on CoMaD by upsampling transitions, predicts the reaching human's pose faster than tracking the current pose.
    </p>

    <br><br><br>

    <img style="width: 90%;" src="./resources/Fig_Results.png" alt="Results"/>
    <p style="width: 80%;text-align: justify;margin-top: 0.8em;">
        Our forecasting method outperforms baselines on Final Displacement Errors (FDE) on all joints, 
        and more specifically shows great improvements on wrist joints that are critical 
        to the Human-Robot interactions. 
    </p>

    <!-- <div class="grid-container">
        <div class="grid-item">
            <figure> 
                <img src="./resources/Handover_Manicast.gif" alt="MLE">
                <figcaption>Object Handover</figcaption>
            </figure>
        </div>
        <div class="grid-item">
            <figure> 
                <img src="./resources/Stirring_Manicast.gif" alt="SAFE">
                <figcaption>Reactive Stirring</figcaption>
            </figure>
        </div>
        <div class="grid-item center">
            <figure> 
                <img src="./resources/Tabletop_ManiCast.gif" alt="SAFE">
                <figcaption>Tabletop Manipulation</figcaption>
            </figure>
        </div>
    </div> -->

    <br><br><br>
    <hr>

    <h1>Qualitative Results</h1>

    <h2>Reactive Stirring</h2>
    <video height="400px";
    frameBorder="0"; 
    controls="controls";>
        <source src="./resources/Stirring_Manicast.mp4" type="video/mp4"></source>
    </video>
    <p style="width: 80%;text-align: justify;margin-top: 0.8em;">
        While the robot is stirring the pot, the human periodically wants to 
        add more vegetables to the pot. For this interaction to occur comfortably,
        the robot must anticipate the human's intent to enter its workspace and pause 
        its stirring momentarily before resuming when the human plans to exit its 
        workspace. Robot manipulation planning is often slow and leads to 
        an uncomfortable retraction, whereas planning with ManiCast forecasts ensures 
        a faster reaction time both when the human moves to place vegetables and 
        when the human is returning to chopping more vegetables.
    </p>

    <h2>Collaborative Table Setting</h2>
    <video height="450px";
    frameBorder="0"; 
    controls="controls";>
        <source src="./resources/Tabletop_ManiCast.mp4" type="video/mp4"></source>
    </video>
    <p style="width: 80%;text-align: justify;margin-top: 0.8em;">
        The human and robot each have their own respective goals, namely picking up 
        different items from the table which are in close proximity. Carrying out their plans to 
        pick up the items may lead to collisions or uncomfortable interactions. Compared to 
        simply planning robot manipulation based on the current position of the human, incorporating 
        ManiCast forecasts into the planner's cost function ensures safety and smoothness.
    </p>

    <h2>Object Handover</h2>
    <video height="500px";
    frameBorder="1"; 
    controls="controls";>
        <source src="./resources/Handover_Manicast.mp4" type="video/mp4"></source>
    </video>
    <p style="width: 80%;text-align: justify;margin-top: 0.8em;">
        When performing an object handover, humans naturally predict the handover location 
        and move their hand to that spot. In order for a robot to do so, it must forecast 
        the handover location and plan its actions accordingly. Compared to the robot following 
        the human's current pose or planning under the assumption that the human will be moving 
        at a constant velocity, planning with ManiCast forecasts produces seamless and realistic 
        interactions.
    </p>

    <hr>

    <h1>Paper</h1>
    <div class="paper-thumbnail">
        <a href="https://openreview.net/forum?id=rxlokRzNWRq">
            <img class="layered-paper-big" width="100%" src="./resources/preview_corl.png" alt="Paper thumbnail"/>
        </a>
    </div>
    <div class="paper-info">
        <a href="https://openreview.net/forum?id=rxlokRzNWRq"><h3>ManiCast: Collaborative Manipulation <br>with Cost-Aware Human Forecasting</h3></a>
        <p>Kushal Kedia, Prithwish Dan, Atiksh Bhardwaj, Sanjiban Choudhury</p>
<pre><code>@inproceedings{
kedia2023manicast,
title={ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting},
author={Kushal Kedia and Prithwish Dan and Atiksh Bhardwaj and Sanjiban Choudhury},
booktitle={7th Annual Conference on Robot Learning},
year={2023},
url={https://openreview.net/forum?id=rxlokRzNWRq}
}   
</code></pre> 
    </div>

    <br><br>
    <hr>


    <br><br>
</div>

</body>

</html>
